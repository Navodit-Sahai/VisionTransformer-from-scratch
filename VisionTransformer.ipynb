{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM/em5XZbCxCZubKnlRg2y4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Navodit-Sahai/VisionTransformer-from-scratch/blob/main/VisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yQokb4Iqg92P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader as dataloader\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#transformation to PIL to tensor format\n",
        "\n",
        "transformation_operation=torchvision.transforms.Compose([torchvision.transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "12swLhNHh8LE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=torchvision.datasets.MNIST(root=\"./data\",train=True, download=True,transform=transformation_operation)\n",
        "val_dataset=torchvision.datasets.MNIST(root=\"./data\",train=False, download=False,transform=transformation_operation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Hl0B2y_bhN34",
        "outputId": "a08c9de3-f5e7-453c-eb19-20407ff09703"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.95MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 131kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.24MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.96MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define datasets batches\n",
        "train_loader= dataloader(train_dataset,batch_size=64,shuffle=True)\n",
        "val_loader=dataloader(val_dataset,batch_size=64,shuffle=True)"
      ],
      "metadata": {
        "id": "PvzcYFY6hoNq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes=10\n",
        "batch_size=64 #no. of images in 1 single batch\n",
        "num_channels=1\n",
        "img_size=28\n",
        "patch_size=7\n",
        "num_patches=(img_size//batch_size)**2\n",
        "embedding_dim=64\n",
        "attention_heads=4\n",
        "transformer_blocks=4\n",
        "learning_rate=0.001\n",
        "mlp_hidden_nodes=128\n",
        "epochs=5"
      ],
      "metadata": {
        "id": "kiNwjFd6jWOL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 1: Patch_embeddings\n",
        "#Part 2: Transformer encoder\n",
        "#Part 3: MLP_head"
      ],
      "metadata": {
        "id": "9T5hP8g0j44f"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VISION TRANSFORMERS DOESN'T HAVE OVERLAPPING PIXELS => NON OVERLAPPING PATCHES ARE REQUIRED"
      ],
      "metadata": {
        "id": "Rc0_Y8dH4_nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datapoint,label=next(iter(train_loader))\n",
        "print(\"shape of datapoint=\", datapoint.shape)\n",
        "patch_embed=nn.Conv2d(num_channels,embedding_dim,kernel_size=patch_size,stride=patch_size)\n",
        "output_tensor = patch_embed(datapoint)\n",
        "print(\"shape after patch_embed=\", output_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41ylP7_06gGT",
        "outputId": "be929880-8770-4d31-8844-f05dedaeab50"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of datapoint= torch.Size([64, 1, 28, 28])\n",
            "shape after patch_embed= torch.Size([64, 64, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need to flatten the patches- 4 along length and 4 along width=> from index 2"
      ],
      "metadata": {
        "id": "1HhasDgV7a5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here in patch_embed [64 (batch_size)= num of images in 1 batch,64(size of embeddings),4(no. of patches along length),4(no. of patch along width)]"
      ],
      "metadata": {
        "id": "gQp9YAgf7H7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_tensor.flatten(2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD2_kNti7mYw",
        "outputId": "79d235a8-8dd1-4e69-afd5-d8f3e178738a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 64, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to display it in correct form i.e (1.) no. of images in 1 batch (2.) no. of patches in 1 image (3.) dimension of each patch\n",
        "output_tensor.flatten(2).transpose(1,2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW0Yl8v29aaJ",
        "outputId": "2499932d-5519-402a-cadc-244e12b66d0f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 16, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "finally we get 16 patches of 1 single image of dim 7*7 arranged in a 1D array"
      ],
      "metadata": {
        "id": "4Iw6q6qj7tOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 batch= 64 images------> 1 image= 16 patches-------> 1 patch= 64*64 dimension"
      ],
      "metadata": {
        "id": "_WpriiDd7sXT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbeddings(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.patch_embed= nn.Conv2d(num_channels,embedding_dim,kernel_size= patch_size,stride=patch_size) #using stride=patch_size we are assuring that we are producing non overlapping patches of the size patch_size.\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.patch_embed(x)\n",
        "    x=x.flatten(2)\n",
        "    x=x.transpose(1,2)\n",
        "    return x"
      ],
      "metadata": {
        "id": "pxVf9Ib-k5uf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_norm1= nn.LayerNorm(embedding_dim)\n",
        "    self.layer_norm2= nn.LayerNorm(embedding_dim)\n",
        "    self.multihead_attention= nn.MultiheadAttention(embed_dim=embedding_dim,num_heads=attention_heads,batch_first=True)\n",
        "    self.mlp= nn.Sequential(\n",
        "        nn.Linear(embedding_dim,mlp_hidden_nodes), #expanding the dimension\n",
        "        nn.GELU(),# applying GELU activation to the expanded dimensions\n",
        "        nn.Linear(mlp_hidden_nodes,embedding_dim)# converting it to its orginal embedding dimension\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    residual1=x\n",
        "    x= self.layer_norm1(x)\n",
        "    x=self.multihead_attention(x,x,x)[0] #query,key,value\n",
        "    x=x+residual1\n",
        "    residual2=x\n",
        "    x=self.layer_norm2(x)\n",
        "    x=self.mlp(x)\n",
        "    x=x+residual2\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "MFyrNf-z-NDp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_head(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_norm1= nn.LayerNorm(embedding_dim)\n",
        "    self.mlp_head=nn.Linear(embedding_dim,num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.layer_norm1(x)\n",
        "    x=self.mlp_head(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "gyRahSTsF6La"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.patch_embedding =PatchEmbeddings()\n",
        "    #class token need to be passed into MLP_head along with embeddings\n",
        "    self.cls_token=nn.Parameter(torch.randn(1,1,embedding_dim))\n",
        "    #positional embedding need to be passed also\n",
        "    self.positional_embedding=nn.Parameter(torch.randn(1,num_patches+1,embedding_dim)) #patch number should be between (16 patches + 1 because we also passing class token with patches)\n",
        "    self.transformer_blocks=nn.Sequential(*[TransformerEncoder() for _ in range(transformer_blocks)])\n",
        "    self.mlp_head=MLP_head()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.patch_embedding(x)\n",
        "    B=x.size(0)\n",
        "    class_tokens=self.cls_token.expand(B,-1,-1) #we need 1 classification token for each image => 1 cls_token for 1 img/16patches. So for 64 images we require 64 token, 1 cls_token is of dim 64\n",
        "    x=torch.cat([class_tokens,x],dim=1)\n",
        "    x=x+self.positional_embedding\n",
        "    x=self.transformer_blocks(x)\n",
        "    x=x[:,0]\n",
        "    x=self.mlp_head(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "5PYXZkIDHCp0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=VisionTransformer()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "3zB3GAcoUt2h"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train() # Set model to training mode\n",
        "  total_train_loss = 0\n",
        "  correct_train = 0\n",
        "  total_train = 0\n",
        "\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    optimizer.zero_grad() # Zero the parameter gradients\n",
        "    outputs = model(images) # Forward pass\n",
        "    loss = criterion(outputs, labels) # Calculate loss\n",
        "    loss.backward() # Backward pass\n",
        "    optimizer.step() # Optimize\n",
        "\n",
        "    total_train_loss += loss.item()\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total_train += labels.size(0)\n",
        "    correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {total_train_loss/len(train_loader):.4f}, Train Accuracy: {100 * correct_train / total_train:.2f}%')\n",
        "\n",
        "  # Validation phase\n",
        "  model.eval() # Set model to evaluation mode\n",
        "  correct_val = 0\n",
        "  total_val = 0\n",
        "  with torch.no_grad(): # Disable gradient calculation during validation\n",
        "    for images, labels in val_loader:\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total_val += labels.size(0)\n",
        "      correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f'Validation Accuracy: {100 * correct_val / total_val:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRHhL7zIVNKn",
        "outputId": "2b8adc93-7784-4e14-e831-63c41d23e2b8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Train Loss: 0.7677, Train Accuracy: 74.08%\n",
            "Validation Accuracy: 89.04%\n",
            "Epoch [2/5], Train Loss: 0.2866, Train Accuracy: 91.02%\n",
            "Validation Accuracy: 93.02%\n",
            "Epoch [3/5], Train Loss: 0.2073, Train Accuracy: 93.47%\n",
            "Validation Accuracy: 93.28%\n",
            "Epoch [4/5], Train Loss: 0.1659, Train Accuracy: 94.66%\n",
            "Validation Accuracy: 94.47%\n",
            "Epoch [5/5], Train Loss: 0.1394, Train Accuracy: 95.47%\n",
            "Validation Accuracy: 94.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EHd45fN7WqOQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}